#@title ## `pip install`
# Don't forget to restart runtime after installing
%pip install "labelbox[data]" --quiet  # installs all required libraries plus extras required in manipulating annotations (shapely, geojson, numpy, PILLOW, opencv-python, etc.)
%pip install -U kaleido  --quiet # for saving the still figures besides .eps (i.e png, pdf)
%pip install poppler-utils  --quiet   # for exporting to .eps extension
%pip install plotly>=5.13.0    # need ≥5.6 to use ticklabelstep argument, ≥5.8 to use minor ticks. Release history here https://github.com/plotly/plotly.py/releases

#@title ## Base imports
import os
import cmd
import sys
import json
import numpy as np
import pandas as pd
import scipy
import scipy.stats
import statsmodels.api as sm
import statsmodels.formula.api as smf

import skimage
import skimage.io
import PIL
import PIL.Image
import requests
import urllib

import labelbox

import IPython.display
import matplotlib
import matplotlib.pyplot as plt
import plotly
import plotly.express as px
import plotly.graph_objects as go
import plotly.subplots

# Display versions of python packages
pip_versions = %system pip freeze  # uses colab magic to get list from shell
pip_versions_organized = {
    "standard": [pip_version for pip_version in pip_versions if "==" in pip_version],
    "other": [pip_version for pip_version in pip_versions if "==" not in pip_version]
    }
print(f"Python version: {sys.version} \n")  # display version of python itself (i.e. 3.8.10)
cli = cmd.Cmd()
cli.columnize(pip_versions_organized["standard"], displaywidth=800)
cli.columnize(pip_versions_organized["other"], displaywidth=160)

#@title Basic helper functions
colab_ip = %system hostname -I   # uses colab magic to get list from shell
colab_ip = colab_ip[0].strip()   # returns "172.28.0.12"
# Get most possible port names with: !sudo lsof -i -P -n | grep LISTEN
colab_port = 9000                # could use 6000, 8080, or 9000

notebook_filename = filename = requests.get(f"http://{colab_ip}:{colab_port}/api/sessions").json()[0]["name"]

# Avoids scroll-in-the-scroll in the entire Notebook
def resize_colab_cell():
  display(IPython.display.Javascript("google.colab.output.setIframeHeight(0, true, {maxHeight: 10000})"))
get_ipython().events.register("pre_run_cell", resize_colab_cell)


#@markdown ### func `def get_path_to_save(...):`
def get_path_to_save(plot_props:dict=None, file_prefix="", save_filename:str=None, save_in_subfolder:str=None, extension="jpg", dot=".", create_folder_if_necessary=True):
    """Code created myself (Rahul Yerrabelli)"""
    replace_characters = {
        "$": "",
        "\\frac":"",
        "\\mathrm":"",
        "\\left(":"(",  "\\right)":")",
        "\\left[":"[",  "\\right]":"]",
        "\\": "",       "/":"-",
        "{": "(",       "}": ")",
        "<":"",         ">":"",
        "?":"",
        "_":"",
        "^":"",
        "*":"",
        "!":"",
        ":":"-",
        "|":"-",
        ".":"_",
    }

    # define save_filename based on plot_props
    if save_filename is None:
        save_filename = "unnamed"

    save_path = [
                 "outputs",
                f"""{notebook_filename.split(".",1)[0]}""",
                ]
    if save_in_subfolder is not None:
        if isinstance(save_in_subfolder, (list, tuple, set, np.ndarray) ):
            save_path.append(**save_in_subfolder)
        else:  # should be a string then
            save_path.append(save_in_subfolder)
    save_path = os.path.join(*save_path)

    if not os.path.exists(save_path) and create_folder_if_necessary:
        os.makedirs(save_path)
    return os.path.join(save_path, file_prefix+save_filename+dot+extension)


#@title ### Prepare for mounting

mountpoint_folder_name = "drive"  # can be anything, doesn't have to be "drive"
project_path_within_drive = "PythonProjects/SpeculumAnalysis" #@param {type:"string"}
project_path_full = os.path.join("/content/",mountpoint_folder_name,
                        "MyDrive",project_path_within_drive)

%cd {project_path_full}

#@title ## Mount google drive
try:
    import google.colab.drive
    import os, sys
    # Need to move out of google drive directory if going to remount
    %cd
    # drive.mount documentation can be accessed via: drive.mount?
    #Signature: drive.mount(mountpoint, force_remount=False, timeout_ms=120000, use_metadata_server=False)
    google.colab.drive.mount(os.path.join("/content/",mountpoint_folder_name), force_remount=True)  # mounts to a folder called mountpoint_folder_name

    if project_path_full not in sys.path:
        pass
        #sys.path.insert(0,project_path_full)
    %cd {project_path_full}
    
except ModuleNotFoundError:  # in case not run in Google colab
    import traceback
    traceback.print_exc()

# Add your labelbox api key and project
# Labelbox API stored in separate file since it is specific for a labelbox 
#account and shouldn't be committed to git. Contact the 
# team (i.e. Rahul Yerrabelli) in order to access to the data on your own account.
with open("auth/LABELBOX_API_KEY.json", "r") as infile:
  json_data = json.load(infile)
API_KEY = json_data["API_KEY"]
del json_data   # delete sensitive info

PROJECT_ID = "cl2cept1u4ees0zbx6uan5kwa"
DATASET_IDS = {
    "1_1": "cl2cerkwd5gtd0zcahfz98401",  # SpeculumData1__GloveNonsterile
    "1_2": "cl2hu1u8z019a0z823yl5f8gr",  # SpeculumData1__Condom
    "2_1": "cl7183159370n072eammu39e6",  # SpeculumData2_1__MultipleSizeSpecula
    "2_2": None,  # haven't uploaded yet, trials weren't that useful
    "2_3": "cleky2xtu19w3070qezkdbhd9",  # SpeculumData2_3__GloveSterile
}

client = labelbox.Client(api_key=API_KEY)
del API_KEY   # delete sensitive info
project = client.get_project(PROJECT_ID)

# Below code is from labelbox tutorial
# Create a mapping for the colors
hex_to_rgb = lambda hex_color: tuple(
    int(hex_color[i + 1:i + 3], 16) for i in (0, 2, 4))
colors = {
    tool.name: hex_to_rgb(tool.color)
    for tool in labelbox.OntologyBuilder.from_project(project).tools
}

datasets = {}
for short_id, DATASET_ID in DATASET_IDS.items():
    if DATASET_ID is None:
        datasets[short_id]=[None]*7
    else:
        # Alternative way to get dataset
        # dataset = next(client.get_datasets(where=(labelbox.Dataset.name == "SpeculumDataset2_3")))
        dataset = client.get_dataset(DATASET_ID)
        datasets[short_id] = [dataset.uid, dataset.name, dataset.description, dataset.created_at, dataset.updated_at, dataset.row_count, dataset]
datasets_df = pd.DataFrame.from_dict(datasets,orient="index", columns=[
    "uid","name","description","created_at","updated_at","row_count","dataset"
])
datasets = datasets_df["dataset"]

data_dfs = {}
for short_id,dataset in datasets.items():
    if dataset is not None:
        data_rows = dataset.data_rows()
        data_dfs[short_id] = pd.DataFrame([[
            short_id, data_row.external_id, data_row.created_at, data_row.updated_at, data_row.uid,
            data_row.media_attributes["width"], data_row.media_attributes["height"], data_row.media_attributes["mimeType"], data_row.media_attributes["contentLength"]
            ]
            for data_row in data_rows],
            columns=["dataset_short_id","external_id","created_at","updated_at","uid", 
                     "width", "height", "mimeType", "contentLength"]
        )
        data_dfs[short_id] = data_dfs[short_id].sort_values(by="external_id")
data_df = pd.concat(data_dfs).reset_index(drop=True)  # drop index as we don't need it and key info is in the df itself
data_df

import pytz
import datetime
chicago_tz = pytz.timezone("America/Chicago") 

for data_row in datasets["2_3"].data_rows():
    #data_row.update()
    ca = data_row.created_at
    print(type(data_row.created_at))
    print(data_row.external_id, data_row.created_at, data_row.updated_at, data_row.uid)
    datatime_str = data_row.external_id
    dt = datetime.datetime(int(datatime_str[0:4]),int(datatime_str[4:6]),int(datatime_str[6:8]),
                           int(datatime_str[9:11]),int(datatime_str[11:13]),int(datatime_str[13:15]),
                           tzinfo=chicago_tz)
    data_row.update(update_at=dt
            )
    # InvalidAttributeError: Field(s) ''update_at'' not valid on DB type 'DataRow'("Field(s) ''update_at'' not valid on DB type 'DataRow'", None)
    break

image_labels = project.label_generator()
image_labels = image_labels.as_list()
labels_df = pd.DataFrame([[
                           label.data.external_id, 
                           label.annotations[0].value.end.x - label.annotations[0].value.start.x, 
                           label.annotations[0].value.end.y - label.annotations[0].value.start.y, 
                           label.annotations[0].value.start.x, 
                           label.annotations[0].value.start.y, 
                           label.data.url, 
                           label.uid,
                           len(label.annotations), len(label.extra["Reviews"]),   # Annotations should be exactly 1. 
                           label.extra["Has Open Issues"], label.extra["Skipped"], 
                           label.extra["Created At"], label.extra["Updated At"], label.extra["Seconds to Label"], label.extra["Created By"],  # Created By is an email address str
                           label.extra["Agreement"], label.extra["Benchmark Agreement"], label.extra["Benchmark ID"]
                           ] 
                          for label in image_labels],
                         columns=["Filename","x","y", "xstart","ystart","url", "Label ID",
                                  "Ann Ct", "Reviews Ct", "Open Issues", "Skipped", 
                                  "created_at","updated_at","Label Seconds", "Created By", 
                                  "Agreement","Benchmark Agreement","Benchmark ID",])

labels2 = project.export_labels(download = True, start="2022-04-01", end="2023-04-01")
labels3 = [value.copy() for value in labels2 ]

for ind in range(len(labels3)):
    # Simplify "Label" and "Reviews" by removing unnecessary variables and making the necessary ones at the top level
    # Thus, labels3 will be only 2 layers deep.
    if "Label" in labels3[ind]:
        coords = labels3[ind]["Label"]["objects"][0]["bbox"]
        for key, val in coords.items():
            labels3[ind]["Label"+"-"+key] = val
        # URL to download mask. Still has token in it
        labels3[ind]["Label_url"] = labels3[ind]["Label"]["objects"][0]["instanceURI"] 
        del labels3[ind]["Label"]

    # Remove special info ie emails, tokens (except the Label_url for now)
    labels3[ind].pop("Labeled Data", None)  # url with token in it
    labels3[ind].pop("View Label", None)  # url
    labels3[ind].pop("Created By", None)  # has email address
    labels3[ind].pop("Reviews", None)  # empty list

# Download and save image masks from URLs
for ind in range(len(labels3)):
    filename = labels3[ind]["External ID"].rsplit(".",maxsplit=1)[0] + "_label"  # usually an .jpg, but split at just "." to be more robust
    filepath = get_path_to_save(save_filename=filename, extension="png")  # the mask will be png, not jpg
    urllib.request.urlretrieve(labels3[ind]["Label_url"], filepath)

# Remove Label_url before saving as that URL has the Labelbox token in it
labelbox_df = pd.DataFrame.from_dict(labels3).set_index("External ID").drop(columns=["Label_url"])

save_path = "data/v2/02_intermediate/labels_df"
labels_df.to_csv( save_path + ".csv")
labels_df.to_pickle(save_path+".pkl")

#labels_df = pd.read_csv("data/02_intermediate/labels_df.csv", index_col=0)
labels_df = pd.read_pickle("data/v2/02_intermediate/labels_df.pkl")

def handle_opening_distance(x):
    if x=="BROKE":
        return 0
    elif type(x)==str and x.lower() in ["n/a","na","nan"]:
        return np.nan
    else:
        return float(x)

# Made Trial a str because it is not really being used as a numeric variable - better for plotting as it becomes a discrete variable instead of continuous (i.e. for color legend)
speculum_df_raw = pd.read_excel("data/01_raw/SpeculumTrialData_v2_3.xlsx", index_col=0, sheet_name="AllTrialsLongVals", 
                                dtype={
                                    "Overall Num": np.int32, "Day Ct": np.int32, "Day Num": np.int32, "Day Num Ct": np.int32, 
                                    "Set Ct": np.int32, "Trial Ct": np.int32, "Set Trial Ct": np.int32, 
                                    "Trial Num": np.int32, #"mmHg": np.int32,
                                    "Spec Ang": np.int32, "Spec Ht": np.int32, 
                                    # Keep size as str even for sterile sizes like 7 and 7.5 to be consistent
                                    "Size": str, "Trial": str, "Filename": str, "Speculum Type": str
                                    }, 
                                converters={"Opening Distance": handle_opening_distance},
                                )    
# For compatibility with older versions
speculum_df_raw.columns = [col.replace("Vertical","Opening").replace("Height","Distance") for col in speculum_df_raw.columns]

#key_cols = ["Speculum Type","Spec Ang","Spec Ht","Size","Material","Material Type","Method"]
#speculum_df_raw.drop_duplicates(subset=key_cols).reset_index().drop("index",axis=1).reset_index().rename({"index":"Set"},axis=1)
#set_info = speculum_df_raw[key_cols].drop_duplicates().reset_index().rename({"index":"Set"},axis=1)
#speculum_df_raw_with_set = speculum_df_raw.merge(set_info, how="outer",on=key_cols)

speculum_df_notfailed = speculum_df_raw.dropna(axis="index", subset=["Filename"])   # Dropped the rows with failed trials



#@title ### Save intermediate files
path_to_folder = "data/v2/02_intermediate"
speculum_df_raw.to_csv(   os.path.join(path_to_folder, "speculum_df_raw"+".csv"))
speculum_df_raw.to_pickle(os.path.join(path_to_folder, "speculum_df_raw"+".pkl"))
speculum_df_notfailed.to_csv(   os.path.join(path_to_folder, "speculum_df_notfailed"+".csv"))
speculum_df_notfailed.to_pickle(os.path.join(path_to_folder, "speculum_df_notfailed"+".pkl"))

freqs = labels_df["Filename"].value_counts()
freqs = freqs[freqs>1]
if len(freqs) > 0:
    display("Warning: There are images with multiple labels. This is not currently supported, and only the first will be kept. This applies for the following files: " + ", ".join( [f"{filename} ({freq})" for filename, freq in freqs.items()] ))
    display(labels_df[labels_df["Filename"].isin(freqs.index)])

df_long=pd.merge(left=speculum_df_notfailed, right=labels_df, on="Filename")

# Drop duplicates i.e. cases where there are multiple labels for the same image
# Currently, don't support combining labels
df_long = df_long.drop_duplicates(subset=["Filename"], keep="first")

glove_rows = df_long["Material Type"]=="Glove"
# The glove images got rotated 90 degrees. To fix this and clarify the directions of the opening, renaming the columns from x,y to wd and ht.
df_long.loc[ glove_rows,"wd"] = df_long.loc[ glove_rows].y
df_long.loc[ glove_rows,"ht"] = df_long.loc[ glove_rows].x
df_long.loc[ glove_rows,"wd_start"] = df_long.loc[ glove_rows].ystart
df_long.loc[ glove_rows,"ht_start"] = df_long.loc[ glove_rows].xstart

df_long.loc[~glove_rows,"wd"] = df_long.loc[~glove_rows].x
df_long.loc[~glove_rows,"ht"] = df_long.loc[~glove_rows].y
df_long.loc[~glove_rows,"wd_start"] = df_long.loc[~glove_rows].xstart
df_long.loc[~glove_rows,"ht_start"] = df_long.loc[~glove_rows].ystart
df_long = df_long.drop(columns=["x","y","xstart","ystart"])

df_long.head()

# Calculate relative value by dividing by the 0mmHg value
base_mmHg = 0 # mmHg
for ind in df_long["Trial Ct"].unique():   # "Trial Ct" was formerly called "Order". "Set Trial Ct" was formerly called "Trial". "Opening Distance" was formerly called "Vertical Height", then "Opening Height"
    df_long.loc[df_long["Trial Ct"]==ind,"wd_rel"]  = 1- df_long.loc[df_long["Trial Ct"]==ind].wd / df_long.loc[ (df_long["Trial Ct"]==ind) & (df_long["mmHg"]==base_mmHg) ].wd.item()
    df_long.loc[df_long["Trial Ct"]==ind,"ht_rel"]  = 1- df_long.loc[df_long["Trial Ct"]==ind].ht / df_long.loc[ (df_long["Trial Ct"]==ind) & (df_long["mmHg"]==base_mmHg) ].ht.item()
#df_long


df_wide = df_long.pivot(index=
                        ["Day Ct","Set Ct","Trial Ct","Set Trial Ct","Speculum Type","Size","Brand","Material","Material Type","Method","Hand","Spec Ang","Spec Ht","Opening Distance"], 
                        columns="mmHg", values=["wd_rel","ht_rel"]).reset_index("Opening Distance")
df_wide_flat = df_wide.copy()
df_wide_flat.columns = [".".join([str(item) for item in col]).strip(".") for col in df_wide_flat.columns.values]

df_multiindex = df_long.set_index(["Trial Ct","mmHg"])
df_multiindex.head(8)

path_to_folder = "data/v2/03_processed"

df_long.to_csv(   os.path.join(path_to_folder, "combined_df_long"+".csv"))
df_long.to_excel( os.path.join(path_to_folder, "combined_df_long"+".xlsx"))
df_long.to_pickle(os.path.join(path_to_folder, "combined_df_long"+".pkl"))

df_wide.to_csv(   os.path.join(path_to_folder, "combined_df_wide"+".csv"))
df_wide.to_excel( os.path.join(path_to_folder, "combined_df_wide"+".xlsx"))
df_wide.to_pickle(os.path.join(path_to_folder, "combined_df_wide"+".pkl"))

df_wide_flat.to_csv(   os.path.join(path_to_folder, "combined_df_wide_flat"+".csv"))
df_wide_flat.to_excel( os.path.join(path_to_folder, "combined_df_wide_flat"+".xlsx"))
df_wide_flat.to_pickle(os.path.join(path_to_folder, "combined_df_wide_flat"+".pkl"))

df_multiindex.to_excel( os.path.join(path_to_folder, "combined_df_multiindex"+".xlsx"))   # assuming a multiindex wouldn't save well to a csv file
df_multiindex.to_pickle(os.path.join(path_to_folder, "combined_df_multiindex"+".pkl"))  

a=df_long[df_long["Day Ct"]==4]
a[["Day Ct", "Set Ct", "Speculum Type", "Size", "Brand", "Material", "Material Type", "Method", "Spec Ang", "Spec Ht",]].value_counts()

#@title ### Get aggregate dfs
# Group by all the parameters that will be the same across different trials of the same object
consistent_cols = ["Day Ct", "Set Ct", "Speculum Type", "Size", "Brand", "Material", "Material Type", "Method", "Spec Ang", "Spec Ht", "mmHg"]
aggregatable_cols = ["wd","ht","wd_rel","ht_rel", "Opening Distance"]
grouped_trials = df_long[consistent_cols+aggregatable_cols].groupby(consistent_cols)
#display(grouped_trials.describe())

def sem(x, ddof=1):   # ddof=1 to get sample standard deviation, not the population standard deviation (np's default)
    sem = np.std(x, ddof=ddof)/np.sqrt(len(x))

def nonnan(x):
    return x[~np.isnan(x)]

df_agg_long = grouped_trials.agg([np.mean, scipy.stats.sem, np.std, np.min, np.median, np.max, np.count_nonzero], ddof=1).reset_index()

df_agg_long_flat = df_agg_long.copy()
df_agg_long_flat.columns = [".".join(col).strip(".") for col in df_agg_long.columns.values]
#df_agg_long_flat

#@title ### Save aggregate dfs
path_to_folder = "data/v2/04_aggregated"
df_agg_long.to_csv(   os.path.join(path_to_folder, "combined_df_agg_long"+".csv"))
df_agg_long.to_excel( os.path.join(path_to_folder, "combined_df_agg_long"+".xlsx"))
df_agg_long.to_pickle(os.path.join(path_to_folder, "combined_df_agg_long"+".pkl"))
df_agg_long_flat.to_csv(   os.path.join(path_to_folder, "combined_df_agg_long_flat"+".csv"))
df_agg_long_flat.to_pickle(os.path.join(path_to_folder, "combined_df_agg_long_flat"+".pkl"))
